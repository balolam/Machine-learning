{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тест 1 \"The Bag of Words representation\"\n",
    "----------------------------------------\n",
    "\n",
    "Функции извлечения текста\n",
    "\n",
    "# CountVectorizer\n",
    "\n",
    "Конвертирует коллекцию текстовых документов в матрицу векторов содержащих счетчики токенов соответствующего документа."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 338,
=======
   "execution_count": 1,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем векторизатор, что бы преобразовать набор документов в вектора числовых признаков. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 339,
=======
   "execution_count": 2,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8beda400-16a9-43cb-b144-e05859ed2c26"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 339,
=======
     "execution_count": 2,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь скормим набор текстовых документов векторизатору и обучим его, заполнив словарь токенов."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 340,
=======
   "execution_count": 3,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 340,
=======
     "execution_count": 3,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили массив векторов, где каждый вектор представлеят собой один документ.\n",
    "Резмерность векторов одинаковая и равна размеру словаря. Значения представляют сосбой кол-во вхождения соответствующего токена в документ.\n",
    "\n",
    "В данном случае размерность вектора равняется:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 341,
=======
   "execution_count": 4,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
<<<<<<< HEAD
     "execution_count": 341,
=======
     "execution_count": 4,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем получить все токены словаря"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 342,
=======
   "execution_count": 5,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
<<<<<<< HEAD
     "execution_count": 342,
=======
     "execution_count": 5,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniqWords = ([\n",
    "        'and', \n",
    "        'document', \n",
    "        'first', \n",
    "        'is', \n",
    "        'one',\n",
    "        'second', \n",
    "        'the', \n",
    "        'third',\n",
    "        'this'\n",
    "    ])\n",
    "\n",
    "vectorizer.get_feature_names() == uniqWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же, есть возможность узнать под каким индексом, какой токен находится в словаре"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 343,
=======
   "execution_count": 6,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and      -> 0\n",
      "document -> 1\n",
      "first    -> 2\n",
      "is       -> 3\n",
      "one      -> 4\n",
      "second   -> 5\n",
      "the      -> 6\n",
      "third    -> 7\n",
      "this     -> 8\n"
     ]
    }
   ],
   "source": [
    "for word in uniqWords:\n",
    "    print \"{:8}\".format(word), \"->\", vectorizer.vocabulary_.get(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем векторизировать документ используя существующий словарь токенов."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 344,
=======
   "execution_count": 7,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 0, 1]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 344,
=======
     "execution_count": 7,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['This is the document!']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы попытаемся векторизировать документ, который заведомо не содержит слов из существующего словаря, то в таком случае они не будут добавлены в него."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 345,
=======
   "execution_count": 8,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 345,
=======
     "execution_count": 8,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(['Something completely new.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим нулевой вектор, что и поддтверждает то, что словарь токенов не пополнился новыми значениями. (Если бы нам нужно было, допустим, перезаписать словарь, то можно было бы снова воспользоваться методом <B>fit_transform(raw_documents[, y]))</B>\n",
    "\n",
    "Бывают случае, когда нам важно еще и сочетание слов в документе, т.е. мы хотим в результате получить больше информации, тогда следует воспользоваться параметром ngram_range (принимающего кортеж), в котором мы указываем диапазон: $$(min, max)$$ вида $$\\{min, min + 1, ... , max - 1, max\\}$$ извлекаемых n-grams."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 346,
=======
   "execution_count": 9,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
<<<<<<< HEAD
     "execution_count": 346,
=======
     "execution_count": 9,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b')\n",
    "bigram_words = (['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])\n",
    "\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') == bigram_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы попытаемся преобразовать ранее используемый корпус, то увидим, что размерность векторов увеличилась"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 347,
=======
   "execution_count": 10,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 347,
=======
     "execution_count": 10,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_X = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "bigram_X"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 348,
=======
   "execution_count": 11,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'and',\n",
       " u'and the',\n",
       " u'document',\n",
       " u'first',\n",
       " u'first document',\n",
       " u'is',\n",
       " u'is the',\n",
       " u'is this',\n",
       " u'one',\n",
       " u'second',\n",
       " u'second document',\n",
       " u'second second',\n",
       " u'the',\n",
       " u'the first',\n",
       " u'the second',\n",
       " u'the third',\n",
       " u'third',\n",
       " u'third one',\n",
       " u'this',\n",
       " u'this is',\n",
       " u'this the']"
      ]
     },
<<<<<<< HEAD
     "execution_count": 348,
=======
     "execution_count": 11,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно к примеру посмотреть как часто встречается определенное слово (в данном случае <b>\"second\"</b>) в корпусе"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 349,
=======
   "execution_count": 12,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 0])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 349,
=======
     "execution_count": 12,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = bigram_vectorizer.vocabulary_.get('second')\n",
    "bigram_X[:, pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfTransformer\n",
    "\n",
    "Может возникнуть проблема, что слова, которые очень часто встречаются не передают суть документа и не несут никакого особо смысла, тогда логичнее будет воспользоваться <b>TfidfTransformer</b>\n",
    "\n",
    "Частота для токена будет пропорциональной частоте его в даннном документе и обратнопропорционально частоте с которой он встречается в других документах корпуса."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 350,
=======
   "execution_count": 13,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 350,
=======
     "execution_count": 13,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "transformer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим массив векторов, и попробуем посчитать частоты используя <b>TfidfTransformer</b>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 351,
=======
   "execution_count": 14,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85151335,  0.        ,  0.52433293],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ],\n",
       "       [ 0.55422893,  0.83236428,  0.        ],\n",
       "       [ 0.63035731,  0.        ,  0.77630514]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 351,
=======
     "execution_count": 14,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf.toarray()                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно выше, каждый столбец и значение нормаированно, а сами веса (которые были закешированы) мы можем получить так"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 352,
=======
   "execution_count": 15,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  2.25276297,  1.84729786])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 352,
=======
     "execution_count": 15,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "Объеденяет в себе два решения <b>CountVectorizer</b> и <b>TfidfTransformer</b>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 353,
=======
   "execution_count": 16,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
<<<<<<< HEAD
     "execution_count": 353,
=======
     "execution_count": 16,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 354,
=======
   "execution_count": 17,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.43877674,  0.54197657,  0.43877674,  0.        ,\n",
       "         0.        ,  0.35872874,  0.        ,  0.43877674],\n",
       "       [ 0.        ,  0.27230147,  0.        ,  0.27230147,  0.        ,\n",
       "         0.85322574,  0.22262429,  0.        ,  0.27230147],\n",
       "       [ 0.55280532,  0.        ,  0.        ,  0.        ,  0.55280532,\n",
       "         0.        ,  0.28847675,  0.55280532,  0.        ],\n",
       "       [ 0.        ,  0.43877674,  0.54197657,  0.43877674,  0.        ,\n",
       "         0.        ,  0.35872874,  0.        ,  0.43877674]])"
      ]
     },
<<<<<<< HEAD
     "execution_count": 354,
=======
     "execution_count": 17,
>>>>>>> 1eae2e064ce16b77a66fe3ec77bc048f052e9699
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer2.fit_transform(corpus).toarray()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "22c727ab-eb44-4b32-ae9b-0fbc33a9307d",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
